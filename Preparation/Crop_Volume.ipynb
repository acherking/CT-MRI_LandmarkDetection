{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Crop the 3D Volume based on some points"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## To find out the suitable crop size.\n",
    "## based on ROI points\n",
    "import Functions.MyDataset as MyDataset\n",
    "import importlib\n",
    "\n",
    "importlib.reload(MyDataset)\n",
    "\n",
    "name_list, pts = MyDataset.load_data_dir(\"/Volumes/Shawn_HDD/PhD/Project/Date/augmentation_from_matlab/original_augmentation_data/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_diff_x = 0\n",
    "max_diff_y = 0\n",
    "max_diff_z = 0\n",
    "\n",
    "for patient in range(pts.shape[0]):\n",
    "    diff_x = max([abs(pts[patient, 0, 0]-pts[patient, 1, 0]), abs(pts[patient, 0, 0]-pts[patient, 2, 0]),\n",
    "                  abs(pts[patient, 0, 0]-pts[patient, 3, 0])])\n",
    "    max_diff_x = max([max_diff_x, diff_x])\n",
    "    diff_y = max([abs(pts[patient, 0, 1]-pts[patient, 1, 1]), abs(pts[patient, 0, 1]-pts[patient, 2, 1]),\n",
    "                  abs(pts[patient, 0, 1]-pts[patient, 3, 1])])\n",
    "    max_diff_y = max([max_diff_y, diff_y])\n",
    "    diff_z = max([abs(pts[patient, 0, 2]-pts[patient, 1, 2]), abs(pts[patient, 0, 2]-pts[patient, 2, 2]),\n",
    "                  abs(pts[patient, 0, 2]-pts[patient, 3, 2])])\n",
    "    max_diff_z = max([max_diff_z, diff_z])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"max_diff_x: \", max_diff_x)\n",
    "print(\"max_diff_y: \", max_diff_y)\n",
    "print(\"max_diff_z: \", max_diff_z)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Looks like the area is still too big (598, 188, 169)\n",
    "So, we think we can focus on the left and right landmark area separately"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Check the cropped results\n",
    "import numpy as np\n",
    "import importlib\n",
    "import Functions.MyDataset as MyDataset\n",
    "import Functions.Visualization as Visualization\n",
    "\n",
    "JH_aug_mat_path = \"/Volumes/Shawn_HDD/PhD/Project/Date/augmentation_from_matlab/original_augmentation_data/AH_aug_2.mat\"\n",
    "# JH_reshape_vol_mat_path = \"/Volumes/Shawn_SSD/PhD/Project/Date/augmentation_from_matlab/Val/Input/FA_17017030_AugVol_2.mat\"\n",
    "# JH_reshape_pts_mat_path = \"/Volumes/Shawn_SSD/PhD/Project/Date/augmentation_from_matlab/Val/Output/FA_17017030_AugPts_2.mat\"\n",
    "\n",
    "pixel_space = [0.15, 0.15, 0.15]\n",
    "\n",
    "importlib.reload(MyDataset)\n",
    "\n",
    "JH_aug_volume, JH_aug_pts, _ = MyDataset.load_mat_data(JH_aug_mat_path)\n",
    "\n",
    "Visualization.show_pts(JH_aug_volume, JH_aug_pts, pixel_space)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import Functions.MyCrop as MyCrop\n",
    "\n",
    "crop_s = ((50, 50), (50, 50), (50, 50))\n",
    "left_volume, left_points, left_length, right_volume, right_points, right_length = MyCrop.crop_volume(JH_aug_volume, JH_aug_pts, crop_s)\n",
    "flip_right_volume, flip_right_points = MyCrop.flip_volume(right_volume, right_points)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Original Points: \", JH_aug_pts)\n",
    "print(\"Locate back: \", np.append(left_points + left_length, right_points + right_length, axis=0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "importlib.reload(Visualization)\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "pixel_space = [0.15, 0.15, 0.15]\n",
    "\n",
    "Visualization.show_two_landmarks(left_volume, left_points, flip_right_volume, flip_right_points, pixel_space)\n",
    "\n",
    "# for i in range(100):\n",
    "#     left_points[0][2] = i\n",
    "#     left_points[1][2] = i\n",
    "#     flip_right_points[0][2] = i\n",
    "#     flip_right_points[1][2] = i\n",
    "#     Visualization.show_two_landmarks(left_volume, left_points, flip_right_volume, flip_right_points, pixel_space)\n",
    "#     time.sleep(1)\n",
    "#     clear_output(wait=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Crop the augmented volumes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Functions.MyDataset as MyDataset\n",
    "\n",
    "# store cropped volumes into one arrays, and points to another\n",
    "pixel_space = [0.15, 0.15, 0.15]\n",
    "dir_path = \"F:/Data/original_augmentation_data/\"\n",
    "save_volumes_dir = \"F:/Data/cropped/x5050y5050z5050/volumes/\"\n",
    "save_points_dir = \"F:/Data/cropped/x5050y5050z5050/points/\"\n",
    "save_length_dir = \"F:/Data/cropped/x5050y5050z5050/length/\"\n",
    "pat_names = ['AH', 'AZ', 'DE', 'DM', 'DM2', 'DGL', 'FA', 'GE', 'GM', 'GP', 'HB', 'HH',\n",
    "             'JH', 'JM', 'LG', 'LP', 'MJ', 'NV', 'PH', 'SM']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for pat_name in pat_names:\n",
    "    for aug_id in range(1, 51):\n",
    "        # such as: \"/Volumes/Shawn_HDD/PhD/Project/Date/augmentation_from_matlab/original_augmentation_data/AH_aug_1.mat\"\n",
    "        file_path = dir_path + pat_name + \"_aug_\" + str(aug_id) + \".mat\"\n",
    "        print(\"load file: \", file_path)\n",
    "        aug_volume, aug_pts, _ = MyDataset.load_mat_data(file_path)\n",
    "        clear_output(wait=True)\n",
    "        print(\"crop volume for: \", file_path)\n",
    "        left_volume, left_points, left_length, right_volume, right_points, right_length = MyCrop.crop_volume(aug_volume, aug_pts)\n",
    "        flip_right_volume, flip_right_points = MyCrop.flip_volume(right_volume, right_points)\n",
    "        Visualization.show_two_landmarks(left_volume, left_points, flip_right_volume, flip_right_points, pixel_space)\n",
    "        print(\"save the cropped volumes\")\n",
    "        np.save(save_volumes_dir + pat_name + \"_augVolume_\"+str(aug_id)+\"_cropped_left.npy\", left_volume)\n",
    "        np.save(save_volumes_dir + pat_name + \"_augVolume_\"+str(aug_id)+\"_cropped_right.npy\", flip_right_volume)\n",
    "        np.save(save_points_dir + pat_name + \"_augPoints_\"+str(aug_id)+\"_cropped_left.npy\", left_points)\n",
    "        np.save(save_points_dir + pat_name + \"_augLength_\"+str(aug_id)+\"_cropped_left.npy\", left_length)\n",
    "        np.save(save_length_dir + pat_name + \"_augPoints_\"+str(aug_id)+\"_cropped_right.npy\", flip_right_points)\n",
    "        np.save(save_length_dir + pat_name + \"_augLength_\"+str(aug_id)+\"_cropped_right.npy\", right_length)\n",
    "        print(\"Finish cropping: \" + file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine cropped volumes\n",
    "cropped_volumes = []\n",
    "cropped_points = []\n",
    "cropped_length = []\n",
    "\n",
    "for pat_name in pat_names:\n",
    "    for aug_id in range(1, 51):\n",
    "        # such as: \"F:/Data/cropped/x5050y5050z5050/volumes/AH_augVolume_1_cropped_left_100x100x100.npy\"\n",
    "        #          \"F:/Data/cropped/x5050y5050z5050/volumes/AH_augVolume_1_cropped_right_100x100x100.npy\"\n",
    "        #          \"F:/Data/cropped/x5050y5050z5050/points/AH_augPoints_1_cropped_left_100x100x100.npy\"\n",
    "        #          \"F:/Data/cropped/x5050y5050z5050/points/AH_augPoints_1_cropped_right_100x100x100.npy\"\n",
    "        print(\"**************\" + pat_name + \"__\" + str(aug_id) + \"***************\")\n",
    "        cropped_volume_left_path = save_volumes_dir + pat_name + \"_augVolume_\" + str(aug_id) + \"_cropped_left.npy\"\n",
    "        cropped_volume_right_path = save_volumes_dir + pat_name + \"_augVolume_\" + str(aug_id) + \"_cropped_right.npy\"\n",
    "        cropped_points_left_path = save_points_dir + pat_name + \"_augPoints_\" + str(aug_id) + \"_cropped_left.npy\"\n",
    "        cropped_length_left_path = save_length_dir + pat_name + \"_augLength_\" + str(aug_id) + \"_cropped_left.npy\"\n",
    "        cropped_points_right_path = save_points_dir + pat_name + \"_augPoints_\" + str(aug_id) + \"_cropped_right.npy\"\n",
    "        cropped_length_right_path = save_length_dir + pat_name + \"_augLength_\" + str(aug_id) + \"_cropped_right.npy\"\n",
    "        cropped_volume_left = np.load(cropped_volume_left_path)\n",
    "        cropped_volume_right = np.load(cropped_volume_right_path)\n",
    "        cropped_points_left = np.load(cropped_points_left_path)\n",
    "        cropped_length_left = np.load(cropped_length_left_path)\n",
    "        cropped_points_right = np.load(cropped_points_right_path)\n",
    "        cropped_length_right = np.load(cropped_length_right_path)\n",
    "        cropped_volumes.append(cropped_volume_left)\n",
    "        cropped_volumes.append(cropped_volume_right)\n",
    "        cropped_points.append(cropped_points_left)\n",
    "        cropped_points.append(cropped_points_right)\n",
    "        cropped_length.append(cropped_length_left)\n",
    "        cropped_length.append(cropped_length_right)\n",
    "\n",
    "print(len(cropped_volumes))\n",
    "print(len(cropped_points))\n",
    "print(len(cropped_length))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cropped_volumes = np.asarray(cropped_volumes).reshape((2000, 100, 100, 100, 1))\n",
    "cropped_points = np.asarray(cropped_points).reshape((2000, 2, 3))\n",
    "cropped_length = np.asarray(cropped_length).reshape((2000, 2, 3))\n",
    "np.save(\"F:/Data/cropped/cropped_volumes_x5050y5050z5050.npy\", cropped_volumes)\n",
    "np.save(\"F:/Data/cropped/cropped_points_x5050y5050z5050.npy\", cropped_points)\n",
    "np.save(\"F:/Data/cropped/cropped_length_x5050y5050z5050.npy\", cropped_length)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"cropped_volumes shape: \", cropped_volumes.shape)\n",
    "print(\"cropped_points shape: \", cropped_points.shape)\n",
    "print(\"cropped_length shape: \", cropped_length.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the Combined cropped volumes\n",
    "import Functions.Visualization as Visualization\n",
    "\n",
    "pat_id = 0\n",
    "Visualization.show_two_landmarks(cropped_volumes[pat_id].reshape(100,100,100), cropped_points[pat_id],\n",
    "                                 cropped_volumes[pat_id+1].reshape(100,100,100), cropped_points[pat_id+1], pixel_space)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize the Prediction Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
