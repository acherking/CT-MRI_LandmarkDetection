{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cut the Volume in the middle and flip the right side"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cut and Flip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "## Check the cropped results\n",
    "import numpy as np\n",
    "import importlib\n",
    "import Functions.MyDataset as MyDataset\n",
    "import Functions.Visualization as Visualization\n",
    "\n",
    "pt_volume_path = \"/Volumes/Shawn_HDD/PhD/Project/Date/augmentation_from_matlab/Rescaled/176_176_48_PD/DM_17617648_VolPts_1.mat\"\n",
    "\n",
    "pixel_space = [0.15, 0.15, 0.15]\n",
    "\n",
    "importlib.reload(Visualization)\n",
    "\n",
    "pat_aug_volume, pat_aug_pts, _ = MyDataset.load_mat_data(pt_volume_path)\n",
    "\n",
    "Visualization.show_pts(pat_aug_volume, pat_aug_pts, pixel_space)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T04:55:52.585825Z",
     "start_time": "2023-08-02T04:55:49.708274Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "-print(pat_aug_pts)\n",
    "trans_pat_aug_pts = np.copy(pat_aug_pts)\n",
    "trans_pat_aug_pts = trans_pat_aug_pts + 1\n",
    "Visualization.show_pts(pat_aug_volume, trans_pat_aug_pts, pixel_space)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import Functions.MyCrop as MyCrop\n",
    "\n",
    "importlib.reload(MyCrop)\n",
    "\n",
    "left_volume, left_points, right_volume, right_points, left_points_cut = MyCrop.cut_flip_volume(pat_aug_volume, pat_aug_pts)\n",
    "flip_right_volume, flip_right_points = MyCrop.flip_volume(right_volume, right_points)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"volume shape: \", pat_aug_volume.shape)\n",
    "print(\"left volume shape: \", left_volume.shape)\n",
    "print(\"right volume shape: \", right_volume.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "left_points_map_back = np.copy(left_points)\n",
    "left_points_map_back[:, 0] = left_points_map_back[:, 0] + left_points_cut\n",
    "\n",
    "print(\"Original Points: \", pat_aug_pts)\n",
    "print(\"Locate back: \", np.append(left_points_map_back, right_points, axis=0))"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "importlib.reload(Visualization)\n",
    "\n",
    "pixel_space = [0.15, 0.15, 0.15]\n",
    "\n",
    "Visualization.show_two_landmarks(left_volume, left_points, flip_right_volume, flip_right_points, pixel_space)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import Functions.MyDataset as MyDataset\n",
    "\n",
    "pixel_space = [0.15, 0.15, 0.15]\n",
    "size_str = \"17617648\"\n",
    "dir_path = \"F:/Data/rescaled_data/176_176_48_PD/\"\n",
    "save_volumes_dir = \"F:/Data/divided/\" + size_str + \"/volumes/\"\n",
    "save_points_dir = \"F:/Data/divided/\" + size_str + \"/points/\"\n",
    "save_length_dir = \"F:/Data/divided/\" + size_str + \"/length_res/\"\n",
    "pat_names = MyDataset.get_pat_names()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.display import display, clear_output\n",
    "import importlib\n",
    "import Functions.Visualization as Visualization\n",
    "\n",
    "import Functions.MyCrop as MyCrop\n",
    "\n",
    "importlib.reload(MyCrop)\n",
    "\n",
    "for pat_name in pat_names:\n",
    "    clear_output(wait=True)\n",
    "    for aug_id in range(1, 51):\n",
    "        # such as: \"\"\n",
    "        file_path = dir_path + pat_name + \"_\" + size_str + \"_VolPts_\" + str(aug_id) + \".mat\"\n",
    "        print(\"load file: \", file_path)\n",
    "        zoomed_aug_volume, zoomed_aug_pts, zoomed_res = MyDataset.load_mat_data(file_path, with_res=True)\n",
    "        # print(\"divide volume for: \", file_path)\n",
    "        left_volume, left_points, right_volume, right_points, left_cut_length = MyCrop.cut_flip_volume(zoomed_aug_volume, zoomed_aug_pts)\n",
    "        flip_right_volume, flip_right_points = MyCrop.flip_volume(right_volume, right_points)\n",
    "        # if aug_id%10 == 1:\n",
    "        #     Visualization.show_two_landmarks(left_volume, left_points, flip_right_volume, flip_right_points, pixel_space)\n",
    "        # print(\"save the divided volumes\")\n",
    "        np.save(save_volumes_dir + pat_name + \"_\" + size_str + f\"_volume_divided_left_{aug_id}.npy\", left_volume)\n",
    "        np.save(save_volumes_dir + pat_name + \"_\" + size_str + f\"_volume_divided_right_{aug_id}.npy\", flip_right_volume)\n",
    "        np.save(save_points_dir + pat_name + \"_\" + size_str + f\"_pts_divided_left_{aug_id}.npy\", left_points)\n",
    "        np.save(save_points_dir + pat_name + \"_\" + size_str + f\"_pts_divided_right_{aug_id}.npy\", flip_right_points)\n",
    "        np.save(save_length_dir + pat_name + \"_\" + size_str + f\"_res_{aug_id}.npy\", zoomed_res)\n",
    "        np.save(save_length_dir + pat_name + \"_\" + size_str + f\"_length_divided_left_{aug_id}.npy\", left_cut_length)\n",
    "        #print(\"Finish dividing: \" + file_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Combine cropped volumes\n",
    "divided_volumes = []\n",
    "divided_points = []\n",
    "divided_length = []\n",
    "res_array = []\n",
    "\n",
    "for pat_name in pat_names:\n",
    "    for aug_id in range(1, 51):\n",
    "        # such as: \"\"\n",
    "        #          \"\"\n",
    "        #          \"\"\n",
    "        #          \"\"\n",
    "        print(\"**************\" + pat_name + \"__\" + str(aug_id) + \"***************\")\n",
    "        divided_volume_left_path = save_volumes_dir + pat_name + \"_\" + size_str + f\"_volume_divided_left_{aug_id}.npy\"\n",
    "        divided_volume_right_path = save_volumes_dir + pat_name + \"_\" + size_str + f\"_volume_divided_right_{aug_id}.npy\"\n",
    "        divided_points_left_path = save_points_dir + pat_name + \"_\" + size_str + f\"_pts_divided_left_{aug_id}.npy\"\n",
    "        divided_points_right_path = save_points_dir + pat_name + \"_\" + size_str + f\"_pts_divided_right_{aug_id}.npy\"\n",
    "        divided_length_left_path = save_length_dir + pat_name + \"_\" + size_str + f\"_length_divided_left_{aug_id}.npy\"\n",
    "        res_path = save_length_dir + pat_name + \"_\" + size_str + f\"_res_{aug_id}.npy\"\n",
    "        divided_volume_left = np.load(divided_volume_left_path)\n",
    "        divided_volume_right = np.load(divided_volume_right_path)\n",
    "        divided_points_left = np.load(divided_points_left_path)\n",
    "        divided_length_left = np.load(divided_length_left_path)\n",
    "        divided_points_right = np.load(divided_points_right_path)\n",
    "        res = np.load(res_path)\n",
    "        divided_volumes.append(divided_volume_left)\n",
    "        divided_volumes.append(divided_volume_right)\n",
    "        divided_points.append(divided_points_left)\n",
    "        divided_points.append(divided_points_right)\n",
    "        divided_length.append(divided_length_left)\n",
    "        res_array.append(res)\n",
    "\n",
    "print(len(divided_volumes))\n",
    "print(len(divided_points))\n",
    "print(len(divided_length))\n",
    "print(len(res_array))"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(res_array[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "divided_volumes = np.asarray(divided_volumes).reshape((2000, 176, 88, 48, 1))\n",
    "divided_points = np.asarray(divided_points).reshape((2000, 2, 3))\n",
    "divided_length = np.asarray(divided_length).reshape((1000, 1))\n",
    "res_array = np.asarray(res_array).reshape((1000, 1, 3))\n",
    "np.save(\"F:/Data/divided/divided_volumes_\" + size_str + \".npy\", divided_volumes)\n",
    "np.save(\"F:/Data/divided/divided_points_\" + size_str + \".npy\", divided_points)\n",
    "np.save(\"F:/Data/divided/divided_length_\" + size_str + \".npy\", divided_length)\n",
    "np.save(\"F:/Data/divided/res_array_\" + size_str + \".npy\", res_array)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Centre point"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check the new dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Check dataset and Trained Model results\n",
    "import Spartan.support_modules as supporter\n",
    "\n",
    "rescaled_size = (176, 176, 48)\n",
    "str_size = str(rescaled_size[0]) + \"_\" + str(rescaled_size[1]) + \"_\" + str(rescaled_size[2])\n",
    "dataset_dir = \"/Volumes/Shawn_HDD/PhD/Project/Date/augmentation_from_matlab/divided/\"\n",
    "\n",
    "X_train, Y_train, res_train, length_train, X_val, Y_val, res_val, length_val, X_test, Y_test, res_test, length_test = \\\n",
    "    supporter.load_dataset_divide(dataset_dir, rescaled_size, pat_splits=MyDataset.get_pat_splits(static=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T04:13:07.017328Z",
     "start_time": "2023-08-01T04:12:55.370305Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "size_str = f\"{rescaled_size[0]}{rescaled_size[1]}{rescaled_size[2]}\"\n",
    "\n",
    "x_dataset_path = dataset_dir + \"divided_volumes_\" + size_str + \".npy\"\n",
    "y_dataset_path = dataset_dir + \"divided_points_\" + size_str + \".npy\"\n",
    "length_dataset_path = dataset_dir + \"divided_length_\" + size_str + \".npy\"\n",
    "res_dataset_path = dataset_dir + \"res_array_\" + size_str + \".npy\"\n",
    "\n",
    "x_dataset = np.load(x_dataset_path)\n",
    "y_dataset = np.load(y_dataset_path).astype('float32')\n",
    "length_dataset = np.load(length_dataset_path).astype('float32')\n",
    "res_dataset = np.load(res_dataset_path).astype('float32')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T04:16:43.058527Z",
     "start_time": "2023-08-01T04:13:40.725140Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "print(length_dataset[0:10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-28T07:06:56.138828Z",
     "start_time": "2023-06-28T07:06:56.124757Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "y_dataset = np.load(y_dataset_path).astype('float32')\n",
    "\n",
    "# y_dataset[list(range(1,2000,2)), :, [0]] = y_dataset[list(range(1,2000,2)), :, [0]] + 2\n",
    "#\n",
    "# np.save(y_dataset_path, y_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-28T05:46:06.070498Z",
     "start_time": "2023-06-28T05:46:06.040758Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import Functions.MyDataset as MyDataset\n",
    "\n",
    "check_idx = MyDataset.get_idx_from_pat('DM', 1, split=True)\n",
    "print(check_idx)\n",
    "\n",
    "left_points = np.copy(y_dataset[check_idx, :, :])\n",
    "right_points = np.copy(y_dataset[check_idx+1, :, :])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T04:25:32.128479Z",
     "start_time": "2023-08-01T04:25:32.116817Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import Functions.Visualization as Visualization\n",
    "\n",
    "importlib.reload(Visualization)\n",
    "\n",
    "pixel_space = [0.15, 0.15, 0.15]\n",
    "# check_idx = 11\n",
    "print(left_points)\n",
    "print(right_points)\n",
    "\n",
    "Visualization.show_two_landmarks(x_dataset[check_idx, :, :, :, 0], left_points, x_dataset[check_idx+1, :, :, :, 0], right_points, pixel_space)\n",
    "\n",
    "print(left_points)\n",
    "print(right_points)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T04:54:32.808936Z",
     "start_time": "2023-08-02T04:54:31.517694Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Review new augmentation data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import Functions.Visualization as Visualization\n",
    "\n",
    "pt_name = \"AH\"\n",
    "aug_id = 4\n",
    "\n",
    "base_dir = \"F:/Data/augmentation_exp/reduce_size/176x88x48\"\n",
    "file_path = f\"{base_dir}/{pt_name}_176x88x48_{aug_id}.mat\"\n",
    "\n",
    "file = h5py.File(file_path, 'r')\n",
    "\n",
    "left_volume = file.get('augLeftVolRescaled')\n",
    "left_pts = file.get('augLeftPtsRescaled')\n",
    "left_res = file.get('leftRes')\n",
    "left_volsize = file.get('augLeftVolSize')\n",
    "\n",
    "right_volume = file.get('augRightVolRescaled')\n",
    "right_pts = file.get('augRightPtsRescaled')\n",
    "right_res = file.get('rightRes')\n",
    "right_volsize = file.get('augRightVolSize')\n",
    "\n",
    "left_volume = np.array(left_volume).T\n",
    "left_pts = np.array(left_pts).reshape(3, 2).T\n",
    "left_vol_res = np.array(left_res).T\n",
    "left_vol_size = np.array(left_volsize).T\n",
    "\n",
    "right_volume = np.array(right_volume).T\n",
    "right_pts = np.array(right_pts).reshape(3, 2).T\n",
    "right_vol_res = np.array(right_res).T\n",
    "right_vol_size = np.array(right_volsize).T\n",
    "\n",
    "pixel_space = [0.15, 0.15, 0.15]\n",
    "\n",
    "Visualization.show_two_landmarks(left_volume, left_pts, right_volume, right_pts, pixel_space)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Combine cropped volumes\n",
    "divided_volumes = []\n",
    "divided_points = []\n",
    "res_array = []\n",
    "\n",
    "base_dir = \"F:/Data/augmentation_exp/reduce_size/176x88x48\"\n",
    "\n",
    "for pat_name in pat_names:\n",
    "    for aug_id in range(1, 51):\n",
    "        print(\"**************\" + pat_name + \"__\" + str(aug_id) + \"***************\")\n",
    "\n",
    "        file_path = f\"{base_dir}/{pat_name}_176x88x48_{aug_id}.mat\"\n",
    "        file = h5py.File(file_path, 'r')\n",
    "\n",
    "        # load Left\n",
    "        left_volume = file.get('augLeftVolRescaled')\n",
    "        left_pts = file.get('augLeftPtsRescaled')\n",
    "        left_res = file.get('leftRes')\n",
    "        # load Right\n",
    "        right_volume = file.get('augRightVolRescaled')\n",
    "        right_pts = file.get('augRightPtsRescaled')\n",
    "        right_res = file.get('rightRes')\n",
    "\n",
    "        left_volume = np.array(left_volume).T\n",
    "        left_pts = np.array(left_pts).reshape(3, 2).T\n",
    "        left_vol_res = np.array(left_res).T\n",
    "\n",
    "        right_volume = np.array(right_volume).T\n",
    "        right_pts = np.array(right_pts).reshape(3, 2).T\n",
    "        right_vol_res = np.array(right_res).T\n",
    "\n",
    "        divided_volumes.append(left_volume)\n",
    "        divided_volumes.append(right_volume)\n",
    "        divided_points.append(left_pts)\n",
    "        divided_points.append(right_pts)\n",
    "        res_array.append(left_res) # left_res and right_res are the same\n",
    "\n",
    "print(len(divided_volumes))\n",
    "print(len(divided_points))\n",
    "print(len(res_array))\n",
    "\n",
    "print(res_array[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "size_str = 17617648\n",
    "save_dir = \"F:/Data/augmentation_exp/reduce_size\"\n",
    "\n",
    "divided_volumes = np.asarray(divided_volumes).reshape((2000, 176, 88, 48, 1))\n",
    "divided_points = np.asarray(divided_points).reshape((2000, 2, 3))\n",
    "divided_length = np.asarray(divided_length).reshape((1000, 1))\n",
    "res_array = np.asarray(res_array).reshape((1000, 1, 3))\n",
    "np.save(f\"{save_dir}/divided_volumes_{size_str}.npy\", divided_volumes)\n",
    "np.save(f\"{save_dir}/divided_points_{size_str}.npy\", divided_points)\n",
    "np.save(f\"{save_dir}/divided_res_{size_str}.npy\", res_array)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
