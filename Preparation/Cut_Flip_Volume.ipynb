{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cut the Volume in the middle and flip the right side"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cut and Flip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Check the cropped results\n",
    "import numpy as np\n",
    "import importlib\n",
    "import Functions.MyDataset as MyDataset\n",
    "import Functions.Visualization as Visualization\n",
    "\n",
    "pt_volume_path = \"/Volumes/Shawn_HDD/PhD/Project/Date/augmentation_from_matlab/Rescaled/176_176_48_PD/AH_17617648_VolPts_2.mat\"\n",
    "# JH_reshape_vol_mat_path = \"/Volumes/Shawn_SSD/PhD/Project/Date/augmentation_from_matlab/Val/Input/FA_17017030_AugVol_2.mat\"\n",
    "# JH_reshape_pts_mat_path = \"/Volumes/Shawn_SSD/PhD/Project/Date/augmentation_from_matlab/Val/Output/FA_17017030_AugPts_2.mat\"\n",
    "\n",
    "pixel_space = [0.15, 0.15, 0.15]\n",
    "\n",
    "importlib.reload(MyDataset)\n",
    "\n",
    "JH_aug_volume, JH_aug_pts, _ = MyDataset.load_mat_data(pt_volume_path)\n",
    "\n",
    "Visualization.show_pts(JH_aug_volume, JH_aug_pts, pixel_space)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T13:15:10.357008Z",
     "end_time": "2023-04-24T13:15:11.766889Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import Functions.MyCrop as MyCrop\n",
    "\n",
    "importlib.reload(MyCrop)\n",
    "\n",
    "left_volume, left_points, right_volume, right_points, left_points_cut = MyCrop.cut_flip_volume(JH_aug_volume, JH_aug_pts)\n",
    "flip_right_volume, flip_right_points = MyCrop.flip_volume(right_volume, right_points)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T13:15:17.768976Z",
     "end_time": "2023-04-24T13:15:18.228496Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"volume shape: \", JH_aug_volume.shape)\n",
    "print(\"left volume shape: \", left_volume.shape)\n",
    "print(\"right volume shape: \", right_volume.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T13:15:19.303018Z",
     "end_time": "2023-04-24T13:15:19.334120Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "left_points_map_back = np.copy(left_points)\n",
    "left_points_map_back[:, 0] = left_points_map_back[:, 0] + left_points_cut\n",
    "\n",
    "print(\"Original Points: \", JH_aug_pts)\n",
    "print(\"Locate back: \", np.append(left_points_map_back, right_points, axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T13:15:28.832190Z",
     "end_time": "2023-04-24T13:15:28.849415Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "importlib.reload(Visualization)\n",
    "\n",
    "pixel_space = [0.15, 0.15, 0.15]\n",
    "\n",
    "Visualization.show_two_landmarks(left_volume, left_points, flip_right_volume, flip_right_points, pixel_space)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T13:15:38.291414Z",
     "end_time": "2023-04-24T13:15:38.902587Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Functions.MyDataset as MyDataset\n",
    "\n",
    "pixel_space = [0.15, 0.15, 0.15]\n",
    "size_str = \"17617648\"\n",
    "dir_path = \"F:/Data/rescaled_data/176_176_48_PD/\"\n",
    "save_volumes_dir = \"F:/Data/divided/\" + size_str + \"/volumes/\"\n",
    "save_points_dir = \"F:/Data/divided/\" + size_str + \"/points/\"\n",
    "save_length_dir = \"F:/Data/divided/\" + size_str + \"/length_res/\"\n",
    "pat_names = MyDataset.get_pat_names()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import importlib\n",
    "import Functions.Visualization as Visualization\n",
    "\n",
    "import Functions.MyCrop as MyCrop\n",
    "\n",
    "importlib.reload(MyCrop)\n",
    "\n",
    "for pat_name in pat_names:\n",
    "    clear_output(wait=True)\n",
    "    for aug_id in range(1, 51):\n",
    "        # such as: \"\"\n",
    "        file_path = dir_path + pat_name + \"_\" + size_str + \"_VolPts_\" + str(aug_id) + \".mat\"\n",
    "        print(\"load file: \", file_path)\n",
    "        zoomed_aug_volume, zoomed_aug_pts, zoomed_res = MyDataset.load_mat_data(file_path, with_res=True)\n",
    "        # print(\"divide volume for: \", file_path)\n",
    "        left_volume, left_points, right_volume, right_points, left_cut_length = MyCrop.cut_flip_volume(zoomed_aug_volume, zoomed_aug_pts)\n",
    "        flip_right_volume, flip_right_points = MyCrop.flip_volume(right_volume, right_points)\n",
    "        # if aug_id%10 == 1:\n",
    "        #     Visualization.show_two_landmarks(left_volume, left_points, flip_right_volume, flip_right_points, pixel_space)\n",
    "        # print(\"save the divided volumes\")\n",
    "        np.save(save_volumes_dir + pat_name + \"_\" + size_str + f\"_volume_divided_left_{aug_id}.npy\", left_volume)\n",
    "        np.save(save_volumes_dir + pat_name + \"_\" + size_str + f\"_volume_divided_right_{aug_id}.npy\", flip_right_volume)\n",
    "        np.save(save_points_dir + pat_name + \"_\" + size_str + f\"_pts_divided_left_{aug_id}.npy\", left_points)\n",
    "        np.save(save_points_dir + pat_name + \"_\" + size_str + f\"_pts_divided_right_{aug_id}.npy\", flip_right_points)\n",
    "        np.save(save_length_dir + pat_name + \"_\" + size_str + f\"_res_{aug_id}.npy\", zoomed_res)\n",
    "        np.save(save_length_dir + pat_name + \"_\" + size_str + f\"_length_divided_left_{aug_id}.npy\", left_cut_length)\n",
    "        #print(\"Finish dividing: \" + file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine cropped volumes\n",
    "divided_volumes = []\n",
    "divided_points = []\n",
    "divided_length = []\n",
    "res_array = []\n",
    "\n",
    "for pat_name in pat_names:\n",
    "    for aug_id in range(1, 51):\n",
    "        # such as: \"\"\n",
    "        #          \"\"\n",
    "        #          \"\"\n",
    "        #          \"\"\n",
    "        print(\"**************\" + pat_name + \"__\" + str(aug_id) + \"***************\")\n",
    "        divided_volume_left_path = save_volumes_dir + pat_name + \"_\" + size_str + f\"_volume_divided_left_{aug_id}.npy\"\n",
    "        divided_volume_right_path = save_volumes_dir + pat_name + \"_\" + size_str + f\"_volume_divided_right_{aug_id}.npy\"\n",
    "        divided_points_left_path = save_points_dir + pat_name + \"_\" + size_str + f\"_pts_divided_left_{aug_id}.npy\"\n",
    "        divided_points_right_path = save_points_dir + pat_name + \"_\" + size_str + f\"_pts_divided_right_{aug_id}.npy\"\n",
    "        divided_length_left_path = save_length_dir + pat_name + \"_\" + size_str + f\"_length_divided_left_{aug_id}.npy\"\n",
    "        res_path = save_length_dir + pat_name + \"_\" + size_str + f\"_res_{aug_id}.npy\"\n",
    "        divided_volume_left = np.load(divided_volume_left_path)\n",
    "        divided_volume_right = np.load(divided_volume_right_path)\n",
    "        divided_points_left = np.load(divided_points_left_path)\n",
    "        divided_length_left = np.load(divided_length_left_path)\n",
    "        divided_points_right = np.load(divided_points_right_path)\n",
    "        res = np.load(res_path)\n",
    "        divided_volumes.append(divided_volume_left)\n",
    "        divided_volumes.append(divided_volume_right)\n",
    "        divided_points.append(divided_points_left)\n",
    "        divided_points.append(divided_points_right)\n",
    "        divided_length.append(divided_length_left)\n",
    "        res_array.append(res)\n",
    "\n",
    "print(len(divided_volumes))\n",
    "print(len(divided_points))\n",
    "print(len(divided_length))\n",
    "print(len(res_array))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(res_array[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "divided_volumes = np.asarray(divided_volumes).reshape((2000, 176, 88, 48, 1))\n",
    "divided_points = np.asarray(divided_points).reshape((2000, 2, 3))\n",
    "divided_length = np.asarray(divided_length).reshape((1000, 1))\n",
    "res_array = np.asarray(res_array).reshape((1000, 1, 3))\n",
    "np.save(\"F:/Data/divided/divided_volumes_\" + size_str + \".npy\", divided_volumes)\n",
    "np.save(\"F:/Data/divided/divided_points_\" + size_str + \".npy\", divided_points)\n",
    "np.save(\"F:/Data/divided/divided_length_\" + size_str + \".npy\", divided_length)\n",
    "np.save(\"F:/Data/divided/res_array_\" + size_str + \".npy\", res_array)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Centre point"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check the new dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Check dataset and Trained Model results\n",
    "import Spartan.support_modules as supporter\n",
    "\n",
    "rescaled_size = (176, 176, 48)\n",
    "str_size = str(rescaled_size[0]) + \"_\" + str(rescaled_size[1]) + \"_\" + str(rescaled_size[2])\n",
    "dataset_dir = \"/Volumes/Shawn_HDD/PhD/Project/Date/augmentation_from_matlab/divided/\"\n",
    "\n",
    "X_train, Y_train, res_train, length_train, X_val, Y_val, res_val, length_val, X_test, Y_test, res_test, length_test = \\\n",
    "    supporter.load_dataset_divide(dataset_dir, rescaled_size, pat_splits=MyDataset.get_pat_splits(static=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T11:12:18.884982Z",
     "end_time": "2023-04-27T11:16:14.226340Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "size_str = f\"{rescaled_size[0]}{rescaled_size[1]}{rescaled_size[2]}\"\n",
    "\n",
    "x_dataset_path = dataset_dir + \"divided_volumes_\" + size_str + \".npy\"\n",
    "y_dataset_path = dataset_dir + \"divided_points_\" + size_str + \".npy\"\n",
    "length_dataset_path = dataset_dir + \"divided_length_\" + size_str + \".npy\"\n",
    "res_dataset_path = dataset_dir + \"res_array_\" + size_str + \".npy\"\n",
    "\n",
    "x_dataset = np.load(x_dataset_path)\n",
    "y_dataset = np.load(y_dataset_path).astype('float32')\n",
    "length_dataset = np.load(length_dataset_path).astype('float32')\n",
    "res_dataset = np.load(res_dataset_path).astype('float32')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T11:16:52.101866Z",
     "end_time": "2023-04-27T11:19:52.337432Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m pixel_space \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0.15\u001B[39m, \u001B[38;5;241m0.15\u001B[39m, \u001B[38;5;241m0.15\u001B[39m]\n\u001B[1;32m      4\u001B[0m check_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m11\u001B[39m\n\u001B[0;32m----> 6\u001B[0m Visualization\u001B[38;5;241m.\u001B[39mshow_two_landmarks(\u001B[43mx_dataset\u001B[49m[check_idx, :, :, :, \u001B[38;5;241m0\u001B[39m], y_dataset[check_idx, :, :], x_dataset[check_idx\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m, :, :, :, \u001B[38;5;241m0\u001B[39m], y_dataset[check_idx\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m, :, :], pixel_space)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'x_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import Functions.Visualization as Visualization\n",
    "\n",
    "pixel_space = [0.15, 0.15, 0.15]\n",
    "check_idx = 11\n",
    "\n",
    "Visualization.show_two_landmarks(x_dataset[check_idx, :, :, :, 0], y_dataset[check_idx, :, :], x_dataset[check_idx+1, :, :, :, 0], y_dataset[check_idx+1, :, :], pixel_space)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T16:46:48.551902Z",
     "end_time": "2023-04-26T16:46:49.340684Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
