{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Vision Transformer (ViT) model 2D"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5123d0bb19429f40"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.layers as layers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T06:35:52.943547Z",
     "start_time": "2023-08-08T06:35:52.936005Z"
    }
   },
   "id": "e3bb2e0a3e35b25",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T06:39:11.893949Z",
     "start_time": "2023-08-08T06:38:23.574822Z"
    }
   },
   "id": "5a69a6bf335bee75",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T06:39:28.435747Z",
     "start_time": "2023-08-08T06:39:28.418484Z"
    }
   },
   "id": "ac5202406737c4c7",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T06:39:35.533934Z",
     "start_time": "2023-08-08T06:39:32.089721Z"
    }
   },
   "id": "55884140633045ce",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T06:39:44.029677Z",
     "start_time": "2023-08-08T06:39:44.017606Z"
    }
   },
   "id": "f94987afd7d749d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T06:39:45.506352Z",
     "start_time": "2023-08-08T06:39:45.496601Z"
    }
   },
   "id": "ebdc503f80714940",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T06:39:52.496830Z",
     "start_time": "2023-08-08T06:39:47.686080Z"
    }
   },
   "id": "4f29e81204e46a25",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T00:48:31.638816Z",
     "start_time": "2023-08-10T00:48:31.625480Z"
    }
   },
   "id": "508d9d42abfcffc9",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T00:48:35.017850Z",
     "start_time": "2023-08-10T00:48:34.983114Z"
    }
   },
   "id": "4648440fdb6e3204",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "vit_model = create_vit_classifier()\n",
    "vit_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T00:49:00.824172Z",
     "start_time": "2023-08-10T00:49:00.335591Z"
    }
   },
   "id": "9d8677d6a372f772",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vision Transformer (ViT) model 3D"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47fa625f13f9dfa4"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.layers as layers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T02:15:22.268194Z",
     "start_time": "2023-09-20T02:15:13.334153Z"
    }
   },
   "id": "7bebeeffea334854",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "base_dir = \"/Volumes/Shawn_HDD/PhD/Project/Date/augmentation_from_matlab/Cropped/based_on_truth/x7575y7575z5050_trans/s1_test_dis\"\n",
    "X_path = f\"{base_dir}/100x100x100_static_s1testdis_X_val.npy\"\n",
    "Y_path = f\"{base_dir}/100x100x100_static_s1testdis_Y_val.npy\"\n",
    "length_path = f\"{base_dir}/100x100x100_static_s1testdis_length_val.npy\"\n",
    "\n",
    "X_test = np.load(X_path)\n",
    "Y_test = np.load(Y_path)\n",
    "length_test = np.load(length_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T03:33:10.457996Z",
     "start_time": "2023-08-15T03:32:50.657014Z"
    }
   },
   "id": "b3b4e9461de37f68",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import Functions.MyCrop as MyCrop\n",
    "import importlib\n",
    "\n",
    "importlib.reload(MyCrop)\n",
    "\n",
    "crop_layers = np.asarray([[14, 14], [14, 14], [26, 26]])\n",
    "\n",
    "X_test_crop, Y_test_crop, length_test_crop = MyCrop.crop_outside_layers(X_test, Y_test, length_test, crop_layers, keep_blank=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T03:42:55.775040Z",
     "start_time": "2023-08-15T03:42:53.637283Z"
    }
   },
   "id": "abe54f07268bdf72",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import Functions.Visualization as Visualization\n",
    "\n",
    "check_id = 6\n",
    "pixel_space = [0.15, 0.15, 0.15]\n",
    "\n",
    "Visualization.show_two_landmarks(X_test_crop[check_id, :, :, :, 0], Y_test_crop[check_id], X_test_crop[check_id+1, :, :, :, 0], Y_test_crop[check_id+1], pixel_space)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T03:43:02.594187Z",
     "start_time": "2023-08-15T03:42:59.228967Z"
    }
   },
   "id": "b64bfde70a183f70",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# learning_rate = 0.001\n",
    "# weight_decay = 0.0001\n",
    "# batch_size = 256\n",
    "# num_epochs = 100\n",
    "image_size = (72, 72, 48)\n",
    "patch_size = (6, 6, 4)\n",
    "num_patches = (image_size[0] // patch_size[0]) * (image_size[1] // patch_size[1]) * (image_size[2] // patch_size[2])\n",
    "projection_dim = 256\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T02:15:00.556368Z",
     "start_time": "2023-09-20T02:15:00.549394Z"
    }
   },
   "id": "af9e59cc910fbf82",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T02:15:04.405800Z",
     "start_time": "2023-09-20T02:15:04.377046Z"
    }
   },
   "id": "bf41f49238e99905",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class Patches3D(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.extract_volume_patches(\n",
    "            input=images,\n",
    "            ksizes=[1, self.patch_size[0], self.patch_size[1], self.patch_size[2], 1],\n",
    "            strides=[1, self.patch_size[0], self.patch_size[1], self.patch_size[2], 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T02:15:26.577948Z",
     "start_time": "2023-09-20T02:15:26.561766Z"
    }
   },
   "id": "17704bdcbc7ac9a1",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = np.random.choice(range(X_test_crop.shape[0]))\n",
    "sli_idx = (Y_test_crop[idx, 0, 2]-1).astype('int')\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "volume = X_test_crop[idx]\n",
    "plt.imshow(volume[:,:,sli_idx, :])\n",
    "plt.axis(\"off\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T05:28:15.786612Z",
     "start_time": "2023-08-15T05:28:15.644998Z"
    }
   },
   "id": "cc69d0280f62d2ad",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "volumes = X_test_crop[0:2]\n",
    "patches = Patches3D(patch_size)(volumes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T03:45:29.042481Z",
     "start_time": "2023-08-15T03:45:28.982643Z"
    }
   },
   "id": "1e50e9224d41ea9b",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(Y_test_crop[0:2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T03:47:54.334159Z",
     "start_time": "2023-08-15T03:47:54.303314Z"
    }
   },
   "id": "d4fe7dff9cc235ce",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "source": [
    "print(f\"Image size: {image_size}\")\n",
    "print(f\"Patch size: {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T07:01:22.779428Z",
     "start_time": "2023-08-14T07:01:22.734425Z"
    }
   },
   "id": "b5c9d3013979c4d0",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "patches_sli = patches[:, :, :, 5, :]\n",
    "patches_sli = np.reshape(patches_sli, (2, 12, 12, 6, 6, 4))\n",
    "patches_one_sli = patches_sli[:, :, :, :, :, 3]\n",
    "patches_show = tf.reshape(patches_one_sli, (2, 144, 36))\n",
    "\n",
    "print(np.max(patches_show[0]))\n",
    "print(np.min(patches_show[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T03:59:41.204949Z",
     "start_time": "2023-08-15T03:59:41.151192Z"
    }
   },
   "id": "f06598a596a3004a",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "n = 12\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches_show[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (6, 6, 1))\n",
    "    plt.imshow(patch_img.numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T04:00:20.250837Z",
     "start_time": "2023-08-15T04:00:15.777661Z"
    }
   },
   "id": "88bd895f76cf1d1f",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class PatchEncoder3D(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim, if_project):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.if_project = if_project\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        if self.if_project:\n",
    "            encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        else:\n",
    "            encoded = patch + self.position_embedding(positions)\n",
    "        return encoded"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T02:15:38.524216Z",
     "start_time": "2023-09-20T02:15:38.503644Z"
    }
   },
   "id": "22bf41127f17a300",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "class ConvStem(layers.Layer):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.conv_layers = keras.Sequential(\n",
    "            [\n",
    "                layers.Conv3D(kernel_size=3, strides=2, filters=48, padding=\"same\"),\n",
    "                layers.Conv3D(kernel_size=3, strides=2, filters=96, padding=\"same\"),\n",
    "                layers.Conv3D(kernel_size=3, strides=2, filters=192, padding=\"same\"),\n",
    "                layers.Conv3D(kernel_size=3, strides=1, filters=384, padding=\"same\"),\n",
    "                layers.Conv3D(kernel_size=1, strides=1, filters=self.dim, padding=\"same\")\n",
    "            ]\n",
    "        )\n",
    "        # self.conv_0 = layers.Conv3D(kernel_size=3, strides=2, filters=48, padding=\"same\")\n",
    "        # self.conv_1 = layers.Conv3D(kernel_size=3, strides=2, filters=96, padding=\"same\")\n",
    "        # self.conv_2 = layers.Conv3D(kernel_size=3, strides=2, filters=192, padding=\"same\")\n",
    "        # self.conv_3 = layers.Conv3D(kernel_size=3, strides=2, filters=384, padding=\"same\")\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        conv_x = self.conv_layers(images)\n",
    "\n",
    "        print(conv_x.shape)\n",
    "        patch_size = conv_x.shape[1] * conv_x.shape[2] * conv_x.shape[3]\n",
    "\n",
    "        patches = tf.reshape(conv_x, [batch_size, patch_size, self.dim])\n",
    "\n",
    "        return patches"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T03:19:44.496946Z",
     "start_time": "2023-09-20T03:19:44.483111Z"
    }
   },
   "id": "3e6f787cbf28c73",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "if_proj = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T02:16:32.697507Z",
     "start_time": "2023-09-20T02:16:32.636012Z"
    }
   },
   "id": "b629906042e55305",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def create_vit3D_regression(volume_shape, points_num):\n",
    "    inputs = layers.Input(shape=volume_shape)\n",
    "    # Create patches.\n",
    "    # patches = Patches3D(patch_size)(inputs)\n",
    "    patches = ConvStem(projection_dim)(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder3D(486, projection_dim, if_proj)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    # representation = layers.Flatten()(representation)\n",
    "    representation = layers.GlobalAveragePooling1D()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    # logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    # model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    # Regression ouputs\n",
    "    reg = layers.Dense(units=points_num * 3, )(features)\n",
    "    reg = layers.Reshape((points_num, 3))(reg)\n",
    "    model = keras.Model(inputs=inputs, outputs=reg)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T03:19:46.798077Z",
     "start_time": "2023-09-20T03:19:46.789495Z"
    }
   },
   "id": "f5165b25b9414a4e",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "input_shape_3D = (72, 72, 48, 1)\n",
    "model_vit3D = create_vit3D_regression(input_shape_3D, 1)\n",
    "model_vit3D.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T03:19:51.451828Z",
     "start_time": "2023-09-20T03:19:49.468433Z"
    }
   },
   "id": "c2169216c1613def",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4552dd24c990b50a",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
