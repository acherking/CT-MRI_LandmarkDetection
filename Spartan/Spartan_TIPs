sinteractive -p gpu-a100 --gres=gpu:1 -A ***
# sinteractive -p gpu-a100 --gres=gpu:1 --mem=30G --time=(01:30:00?)
sinteractive -p gpu-a100 --gres=gpu:1 --mem=30G --time=01:30:00

-p: partition
--gres: generic resources
-A: account group, e.g.punim0006
--time 00:05:00


module purge
module load cuda/11.1.1
module load tensorflow/2.6.0-python-3.8.6

# It seems this version was not compiled to use GPU
module load tensorflow/2.7.1

***Python***

import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))


# To show which device dit the operations
tf.debugging.set_log_device_placement(True)

# Data Parallelism basics?
## 1. divide batch: batch-size=10, 2 GPUs, each batch of size 10 will be divided among the 2 GPUs, with each receiving 5 input examples in each step.

*** Conda 
# activate Env
. /Users/achs/opt/anaconda3/bin/activate && conda activate /Users/achs/opt/anaconda3/envs/test;