sinteractive -p gpu-a100 --gres=gpu:1 -A ***

-p: partition
--gres: generic resources
-A: account group, e.g.punim0006
--time 00:05:00


module purge
module load fosscuda/2019b
module load tensorflow/2.1.0-python-3.7.4

***Python***

import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))


# To show which device dit the operations
tf.debugging.set_log_device_placement(True)

# Data Parallelism basics?
## 1. divide batch: batch-size=10, 2 GPUs, each batch of size 10 will be divided among the 2 GPUs, with each receiving 5 input examples in each step.