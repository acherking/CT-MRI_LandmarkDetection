{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TensorFlow Keras, Layers Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import tensorflow as tf"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# The inputs are 28x28 RGB images with `channels_last` and the batch\n",
    "# size is 4.\n",
    "input_shape = (4, 28, 28, 3)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv2D(\n",
    "    2, 3, strides=1, activation='relu', input_shape=input_shape[1:])(x)\n",
    "print(y.shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# With `dilation_rate` as 2.\n",
    "input_shape = (4, 28, 28, 3)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv2D(\n",
    "    2, 3,\n",
    "    activation='relu',\n",
    "    dilation_rate=2,\n",
    "    input_shape=input_shape[1:])(x)\n",
    "print(y.shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    " # With `padding` as \"same\".\n",
    "input_shape = (4, 28, 28, 3)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv2D(\n",
    "    2, 3, activation='relu', padding=\"same\", input_shape=input_shape[1:])(x)\n",
    "print(y.shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# With extended batch shape [4, 7]:\n",
    "# input_shape = (4, 7, 28, 28, 3)\n",
    "# x = tf.random.normal(input_shape)\n",
    "# y = tf.keras.layers.Conv2D(\n",
    "#     2, 3, activation='relu', input_shape=input_shape[2:])(x)\n",
    "# print(y.shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TensorFlow Keras, Layers Conv3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# The inputs are 28x28x28 volumes with a single channel, and the\n",
    "# batch size is 4\n",
    "input_shape =(4, 28, 28, 28, 1)\n",
    "x = tf.random.normal(input_shape)\n",
    "x = tf.keras.layers.Conv3D(\n",
    "    2, 3, activation='relu', input_shape=input_shape[1:])(x)\n",
    "print(x.shape)\n",
    "x = tf.keras.layers.Conv3D(\n",
    "    4, 3, activation='relu')(x)\n",
    "print(x.shape)\n",
    "y = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "print(y.shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# # With extended batch shape [4, 7], e.g. a batch of 4 videos of\n",
    "# # 3D frames, with 7 frames per video.\n",
    "# input_shape = (4, 7, 28, 28, 28, 1)\n",
    "# x = tf.random.normal(input_shape)\n",
    "# y = tf.keras.layers.Conv3D(\n",
    "#     2, 3, activation='relu', input_shape=input_shape[2:])(x)\n",
    "# print(y.shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Try to build a 3D convolutional neural network for the Landmark Detection project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROI_CT_path = '../Resources/ROI_CT.xlsx'\n",
    "ROI_MR_path = '../Resources/ROI_MR.xlsx'\n",
    "\n",
    "roi_CT = pd.read_excel(ROI_CT_path, index_col=[0, 1], header=[0, 1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "pname_list = list(set([x for (x, y) in roi_CT.index]))\n",
    "pname_list.sort()\n",
    "\n",
    "print(pname_list)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "pixel_space = [0.15, 0.15, 0.15]\n",
    "data_path_base = \"/Volumes/Shawn_HDD/PhD/Project/Date/CT_Augmented/\"\n",
    "\n",
    "p_name = pname_list[0]\n",
    "data_path = data_path_base + p_name + \"_aug_0.npy\"\n",
    "volume_data = np.load(data_path, allow_pickle=True).item()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import functions\n",
    "\n",
    "functions.show_pts(volume_data.get(\"volume\"), volume_data.get(\"pts\"), pixel_space)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load data from .mat --- train, test, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import h5py"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "# e.g. AZ_17017030_AugVol_1.mat\n",
    "X_train_base_path = \"/Volumes/Shawn_SSD/PhD/Project/Date/augmentation_from_matlab/Train/Input/\"\n",
    "# e.g. AZ_17017030_AugPts_1.mat\n",
    "Y_train_base_path = \"/Volumes/Shawn_SSD/PhD/Project/Date/augmentation_from_matlab/Train/Output/\"\n",
    "\n",
    "X_train_files = [f for f in listdir(X_train_base_path) if isfile(join(X_train_base_path, f))]\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for X_train_file in X_train_files:\n",
    "    X_train_file_path = join(X_train_base_path, X_train_file)\n",
    "    Y_train_file_path = join(Y_train_base_path, X_train_file.replace(\"AugVol\", \"AugPts\"))\n",
    "    file_vol = h5py.File(X_train_file_path, 'r')\n",
    "    file_pts = h5py.File(Y_train_file_path, 'r')\n",
    "    load_mat_vol = file_vol.get('rescaled_aug_vol')\n",
    "    load_mat_pts = file_pts.get('rescaled_aug_pts')\n",
    "    X_train.append(np.array(load_mat_vol).T)\n",
    "    Y_train.append(np.array(load_mat_pts).reshape(3,4).T)\n",
    "    file_vol.close()\n",
    "    file_pts.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "# e.g. AZ_17017030_AugVol_1.mat\n",
    "X_val_base_path = \"/Volumes/Shawn_SSD/PhD/Project/Date/augmentation_from_matlab/Val/Input/\"\n",
    "# e.g. AZ_17017030_AugPts_1.mat\n",
    "Y_val_base_path = \"/Volumes/Shawn_SSD/PhD/Project/Date/augmentation_from_matlab/Val/Output/\"\n",
    "\n",
    "X_val_files = [f for f in listdir(X_val_base_path) if isfile(join(X_val_base_path, f))]\n",
    "\n",
    "X_val = []\n",
    "Y_val = []\n",
    "for X_val_file in X_val_files:\n",
    "    X_val_file_path = join(X_val_base_path, X_val_file)\n",
    "    Y_val_file_path = join(Y_val_base_path, X_val_file.replace(\"AugVol\", \"AugPts\"))\n",
    "    file_vol = h5py.File(X_val_file_path, 'r')\n",
    "    file_pts = h5py.File(Y_val_file_path, 'r')\n",
    "    load_mat_vol = file_vol.get('rescaled_aug_vol')\n",
    "    load_mat_pts = file_pts.get('rescaled_aug_pts')\n",
    "    X_val.append(np.array(load_mat_vol).T)\n",
    "    Y_val.append(np.array(load_mat_pts).reshape(3,4).T)\n",
    "    file_vol.close()\n",
    "    file_pts.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(Y_train))\n",
    "\n",
    "print(np.shape(X_val))\n",
    "print(np.shape(Y_val))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "X_train_reshape = np.asarray(X_train).reshape(700, 170, 170, 30, 1)\n",
    "Y_train = np.asarray(Y_train)[:, 0, :]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "X_val_reshape = np.asarray(X_val).reshape(100, 170, 170, 30, 1)\n",
    "Y_val = np.asarray(Y_val)[:, 0, :]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print(Y_train.shape)\n",
    "print(X_train_reshape.shape)\n",
    "\n",
    "print(Y_val.shape)\n",
    "print(X_val_reshape.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_reshape, Y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1400).batch(2)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_reshape, Y_val))\n",
    "val_dataset = val_dataset.shuffle(buffer_size=200).batch(2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "for step, (x, y) in enumerate(train_dataset):\n",
    "    print(\"step: \", step)\n",
    "    print(\"x shape: \", x.shape, type(x))\n",
    "    print(\"y shape: \", y.shape, type(y))\n",
    "    print(y.shape)\n",
    "    break\n",
    "\n",
    "\n",
    "for step, (x, y) in enumerate(val_dataset):\n",
    "    print(\"step: \", step)\n",
    "    print(\"x shape: \", x.shape, type(x))\n",
    "    print(\"y shape: \", y.shape, type(y))\n",
    "    print(y.shape)\n",
    "    break"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Define data loaders.\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((X_train_reshape, Y_train))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((X_val_reshape, Y_val))\n",
    "\n",
    "batch_size = 2\n",
    "train_dataset = (\n",
    "    train_loader.shuffle(len(X_train_reshape))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "# Only rescale.\n",
    "validation_dataset = (\n",
    "    validation_loader.shuffle(len(X_val))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "def get_model(width=170, height=170, depth=30):\n",
    "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
    "\n",
    "    inputs = keras.Input((width, height, depth, 1))\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "    #x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "    #x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(units=3, )(x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "    return model\n",
    "\n",
    "# Build model.\n",
    "model = get_model(width=170, height=170, depth=30)\n",
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Compile model.\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "model.compile(\n",
    "    loss=\"mean_squared_error\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    #metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "# Define callbacks.\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"3d_image_classification.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch\n",
    "epochs = 100\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
