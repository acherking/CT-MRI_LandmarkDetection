{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to Keras for Researchers --- [origin](https://keras.io/getting_started/intro_to_keras_for_researchers/)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "x = tf.constant([[5, 2], [1, 3]])\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "x.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print(\"dtype:\", x.dtype)\n",
    "print(\"shape:\", x.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(tf.ones(shape=(2, 1)))\n",
    "print(tf.zeros(shape=(2, 1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "x = tf.random.normal(shape=(2, 2), mean=0.0, stddev=1.0)\n",
    "print(x)\n",
    "x = tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype=\"int32\")\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "initial_value = tf.random.normal(shape=(2, 2))\n",
    "print(initial_value)\n",
    "a = tf.Variable(initial_value)\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "new_value = tf.random.normal(shape=(2, 2))\n",
    "print(\"new value: \", new_value)\n",
    "a.assign(new_value)\n",
    "print(\"after assign, a: \", a)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i, j] == new_value[i, j]\n",
    "print(\"after for, a: \", a)\n",
    "\n",
    "added_value = tf.random.normal(shape=(2, 2))\n",
    "print(\"added value: \", added_value)\n",
    "a.assign_add(added_value)\n",
    "print(\"after add, a: \", added_value)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i, j] == new_value[i, j] + added_value[i, j]\n",
    "print(\"after for, a: \", added_value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "c = a + b\n",
    "d = tf.square(c)\n",
    "e = tf.exp(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(a)  # Start recording the history of operations applied to `a`\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))  # Do some math using `a`\n",
    "    # What's the gradient of `c` with respect to `a`?\n",
    "    dc_da = tape.gradient(c, a)\n",
    "    print(dc_da)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "a / tf.sqrt(tf.square(a) + tf.square(b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "a = tf.Variable(a)\n",
    "\n",
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as tape:\n",
    "        c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "        dc_da = tape.gradient(c, a)\n",
    "    d2c_da2 = outer_tape.gradient(dc_da, a)\n",
    "    print(d2c_da2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keras layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w1 = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.w2 = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w1) + self.b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Instantiate our layer.\n",
    "linear_layer = Linear(units=4, input_dim=2)\n",
    "\n",
    "# The layer can be treated as a function.\n",
    "# Here we call it on some data.\n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "assert y.shape == (2, 4)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "assert linear_layer.weights == [linear_layer.w1, linear_layer.w2, linear_layer.b]"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "print(linear_layer.weights)\n",
    "print(linear_layer.w1)\n",
    "print(linear_layer.w2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "a = tf.ones(shape=(2,4))\n",
    "print(\"a: \", a)\n",
    "b = tf.constant([1.,2.,3.,4.])\n",
    "print(b.shape)\n",
    "\n",
    "print(a+b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "\n",
    "# Instantiate our lazy layer.\n",
    "linear_layer = Linear(4)\n",
    "\n",
    "# This will also call `build(input_shape)` and create the weights.\n",
    "y = linear_layer(tf.ones((2, 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Instantiate our linear layer (defined above) with 10 units.\n",
    "linear_layer = Linear(10)\n",
    "\n",
    "# Instantiate a logistic loss function that expects integer targets.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "# Iterate over the batches of the dataset.\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "\n",
    "    # Open a GradientTape.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Forward pass.\n",
    "        logits = linear_layer(x)\n",
    "\n",
    "        # Loss value for this batch.\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "    # Get gradients of the loss wrt the weights.\n",
    "    gradients = tape.gradient(loss, linear_layer.trainable_weights)\n",
    "\n",
    "    # Update the weights of our linear layer.\n",
    "    optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))\n",
    "\n",
    "    # Logging.\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ComputeSum(keras.layers.Layer):\n",
    "    \"\"\"Returns the sum of the inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        # Create a non-trainable weight.\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "\n",
    "\n",
    "my_sum = ComputeSum(2)\n",
    "x = tf.ones((2, 2))\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [2. 2.]\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [4. 4.]\n",
    "\n",
    "assert my_sum.weights == [my_sum.total]\n",
    "assert my_sum.non_trainable_weights == [my_sum.total]\n",
    "assert my_sum.trainable_weights == []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's reuse the Linear class\n",
    "# with a `build` method that we defined above.\n",
    "\n",
    "\n",
    "class MLP(keras.layers.Layer):\n",
    "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = MLP()\n",
    "\n",
    "# The first call to the `mlp` object will create the weights.\n",
    "y = mlp(tf.ones(shape=(3, 64)))\n",
    "\n",
    "# Weights are recursively tracked.\n",
    "assert len(mlp.weights) == 6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlp = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ActivityRegularization(keras.layers.Layer):\n",
    "    \"\"\"Layer that creates an activity sparsity regularization loss.\"\"\"\n",
    "\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularization, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # We use `add_loss` to create a regularization loss\n",
    "        # that depends on the inputs.\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's use the loss layer in a MLP block.\n",
    "\n",
    "\n",
    "class SparseMLP(keras.layers.Layer):\n",
    "    \"\"\"Stack of Linear layers with a sparsity regularization loss.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SparseMLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.regularization = ActivityRegularization(1e-2)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.regularization(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = SparseMLP()\n",
    "y = mlp(tf.ones((10, 10)))\n",
    "\n",
    "print(mlp.losses)  # List containing one float32 scalar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "print(y_train[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# A new MLP.\n",
    "mlp = SparseMLP()\n",
    "\n",
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Forward pass.\n",
    "        logits = mlp(x)\n",
    "\n",
    "        # External loss value for this batch.\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "        # Add the losses created during the forward pass.\n",
    "        loss += sum(mlp.losses)\n",
    "\n",
    "        # Get gradients of the loss wrt the weights.\n",
    "        gradients = tape.gradient(loss, mlp.trainable_weights)\n",
    "\n",
    "    # Update the weights of our linear layer.\n",
    "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
    "\n",
    "    # Logging.\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Instantiate a metric object\n",
    "accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Prepare our layer, loss, and optimizer.\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(2):\n",
    "    # Iterate over the batches of a dataset.\n",
    "    for step, (x, y) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            # Compute the loss value for this batch.\n",
    "            loss_value = loss_fn(y, logits)\n",
    "\n",
    "        # Update the state of the `accuracy` metric.\n",
    "        accuracy.update_state(y, logits)\n",
    "\n",
    "        # Update the weights of the model to minimize the loss value.\n",
    "        gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "        # Logging the current accuracy value so far.\n",
    "        if step % 200 == 0:\n",
    "            print(\"Epoch:\", epoch, \"Step:\", step)\n",
    "            print(\"Total running accuracy so far: %.3f\" % accuracy.result())\n",
    "\n",
    "    # Reset the metric's state at the end of an epoch\n",
    "    accuracy.reset_states()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class F1Score(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"f1_score\", dtype=\"float32\", threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, dtype=dtype, **kwargs)\n",
    "        self.threshold = 0.5\n",
    "        self.true_positives = self.add_weight(\n",
    "            name=\"tp\", dtype=dtype, initializer=\"zeros\"\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name=\"fp\", dtype=dtype, initializer=\"zeros\"\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name=\"fn\", dtype=dtype, initializer=\"zeros\"\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.math.greater_equal(y_pred, self.threshold)\n",
    "        y_true = tf.cast(y_true, tf.bool)\n",
    "        y_pred = tf.cast(y_pred, tf.bool)\n",
    "\n",
    "        true_positives = tf.cast(y_true & y_pred, self.dtype)\n",
    "        false_positives = tf.cast(~y_true & y_pred, self.dtype)\n",
    "        false_negatives = tf.cast(y_true & ~y_pred, self.dtype)\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, self.dtype)\n",
    "            true_positives *= sample_weight\n",
    "            false_positives *= sample_weight\n",
    "            false_negatives *= sample_weight\n",
    "\n",
    "        self.true_positives.assign_add(tf.reduce_sum(true_positives))\n",
    "        self.false_positives.assign_add(tf.reduce_sum(false_positives))\n",
    "        self.false_negatives.assign_add(tf.reduce_sum(false_negatives))\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.true_positives / (self.true_positives + self.false_positives)\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives)\n",
    "        return precision * recall * 2.0 / (precision + recall)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.false_positives.assign(0)\n",
    "        self.false_negatives.assign(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "m = F1Score()\n",
    "m.update_state([0, 1, 0, 0], [0.3, 0.5, 0.8, 0.9])\n",
    "print(\"Intermediate result:\", float(m.result()))\n",
    "\n",
    "m.update_state([1, 1, 1, 1], [0.1, 0.7, 0.6, 0.0])\n",
    "print(\"Final result:\", float(m.result()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Prepare our layer, loss, and optimizer.\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Create a training step function.\n",
    "\n",
    "\n",
    "@tf.function  # Make it fast.\n",
    "def train_on_batch(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(y, logits)\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_on_batch(x, y)\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Dropout(keras.layers.Layer):\n",
    "    def __init__(self, rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class MLPWithDropout(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPWithDropout, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = MLPWithDropout()\n",
    "y_train = mlp(tf.ones((2, 2)), training=True)\n",
    "y_test = mlp(tf.ones((2, 2)), training=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We use an `Input` object to describe the shape and dtype of the inputs.\n",
    "# This is the deep learning equivalent of *declaring a type*.\n",
    "# The shape argument is per-sample; it does not include the batch size.\n",
    "# The functional API focused on defining per-sample transformations.\n",
    "# The model we create will automatically batch the per-sample transformations,\n",
    "# so that it can be called on batches of data.\n",
    "inputs = tf.keras.Input(shape=(16,), dtype=\"float32\")\n",
    "\n",
    "# We call layers on these \"type\" objects\n",
    "# and they return updated types (new shapes/dtypes).\n",
    "x = Linear(32)(inputs)  # We are reusing the Linear layer we defined earlier.\n",
    "x = Dropout(0.5)(x)  # We are reusing the Dropout layer we defined earlier.\n",
    "outputs = Linear(10)(x)\n",
    "\n",
    "# A functional `Model` can be defined by specifying inputs and outputs.\n",
    "# A model is itself a layer like any other.\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# A functional model already has weights, before being called on any data.\n",
    "# That's because we defined its input shape in advance (in `Input`).\n",
    "assert len(model.weights) == 4\n",
    "\n",
    "# Let's call our model on some data, for fun.\n",
    "y = model(tf.ones((2, 16)))\n",
    "assert y.shape == (2, 10)\n",
    "\n",
    "# You can pass a `training` argument in `__call__`\n",
    "# (it will get passed down to the Dropout layer).\n",
    "y = model(tf.ones((2, 16)), training=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inputs = tf.keras.Input(shape=(784,), dtype=\"float32\")\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(inputs)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "outputs = keras.layers.Dense(10)(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Specify the loss, optimizer, and metrics with `compile()`.\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train the model with the dataset for 2 epochs.\n",
    "model.fit(dataset, epochs=2)\n",
    "model.predict(dataset)\n",
    "model.evaluate(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
