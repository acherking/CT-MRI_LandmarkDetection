{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46b55b5-c5ea-4fc2-912f-598c29942e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b335dfc5-76af-4c75-aeec-8080058dc7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_dataset_divide(dataset_dir, rescaled_size, idx_splits, no_split=False):\n",
    "    size_str = f\"{rescaled_size[0]}{rescaled_size[1]}{rescaled_size[2]}\"\n",
    "\n",
    "    x_dataset_path = dataset_dir + \"divided_volumes_\" + size_str + \".npy\"\n",
    "    y_dataset_path = dataset_dir + \"divided_points_\" + size_str + \".npy\"\n",
    "    res_dataset_path = dataset_dir + \"divided_res_\" + size_str + \".npy\"\n",
    "\n",
    "    x_dataset = np.load(x_dataset_path).astype('float32')\n",
    "    y_dataset = np.load(y_dataset_path).astype('float32')\n",
    "    res_dataset = np.load(res_dataset_path).astype('float32')\n",
    "\n",
    "    res_dataset_rep = np.repeat(res_dataset, 2, axis=1).reshape(2000, 1, 3)\n",
    "\n",
    "    # without splitting to Train, Val and Test\n",
    "    if no_split:\n",
    "        return x_dataset, y_dataset, res_dataset_rep\n",
    "\n",
    "    train_idx = idx_splits[0]\n",
    "    x_train = x_dataset[train_idx]\n",
    "    y_train = y_dataset[train_idx]\n",
    "    res_train = res_dataset_rep[train_idx]\n",
    "\n",
    "    val_idx = idx_splits[1]\n",
    "    x_val = x_dataset[val_idx]\n",
    "    y_val = y_dataset[val_idx]\n",
    "    res_val = res_dataset_rep[val_idx]\n",
    "\n",
    "    test_idx = idx_splits[2]\n",
    "    x_test = x_dataset[test_idx]\n",
    "    y_test = y_dataset[test_idx]\n",
    "    res_test = res_dataset_rep[test_idx]\n",
    "\n",
    "    return x_train, y_train, res_train, \\\n",
    "        x_val, y_val, res_val, \\\n",
    "        x_test, y_test, res_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9e29f01-7a84-452e-a9ab-da1eeb7788e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(pat_splits, split=False, aug_num=50):\n",
    "\n",
    "    if split:\n",
    "        double_aug_num = aug_num * 2\n",
    "        idx_splits = [[list(range(i * double_aug_num, i * double_aug_num + double_aug_num)) for i in j] for j in pat_splits]\n",
    "        for i in range(0, 3):\n",
    "            idx_splits[i] = [num for sublist in idx_splits[i] for num in sublist]\n",
    "            idx_splits[i] = np.asarray(idx_splits[i])\n",
    "    else:\n",
    "        idx_splits = [[list(range(i * aug_num, i * aug_num + aug_num)) for i in j] for j in pat_splits]\n",
    "        for i in range(0, 3):\n",
    "            idx_splits[i] = [num for sublist in idx_splits[i] for num in sublist]\n",
    "            idx_splits[i] = np.asarray(idx_splits[i])\n",
    "\n",
    "    return idx_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee4acb8-7806-4dc5-bc51-d8a087f9f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_splits = [np.asarray([2, 4, 18, 17, 12, 10, 6, 0, 11, 16, 9, 14, 5, 19]), np.asarray([3, 13]), np.asarray([8, 7, 1, 15])]\n",
    "dataset_dir = \"/data/gpfs/projects/punim1836/Data/divided/17617648/\"\n",
    "rescaled_size = (176, 176, 48)\n",
    "data_splits = get_data_splits(pat_splits, split=True, aug_num=50)\n",
    "\n",
    "x_train, y_train, res_train, x_val, y_val, res_val, x_test, y_test, res_test = load_dataset_divide(dataset_dir, rescaled_size, data_splits)\n",
    "\n",
    "# move channel forward\n",
    "x_train = np.transpose(x_train, (0,4,1,2,3))\n",
    "x_val = np.transpose(x_val, (0,4,1,2,3))\n",
    "x_test = np.transpose(x_test, (0,4,1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e1c5743-c626-4dee-ac2d-5510983534c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_size, row_size, slice_size = 88, 176, 48\n",
    "\n",
    "res_train = (res_train / [2/column_size, 2/row_size, 2/slice_size]).astype('float32')\n",
    "res_val = (res_val / [2/column_size, 2/row_size, 2/slice_size]).astype('float32')\n",
    "res_test = (res_test / [2/column_size, 2/row_size, 2/slice_size]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2146cdb0-9fb5-45bd-a77f-29d61d9a2682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the Y\n",
    "image_size = [88, 176, 48]\n",
    "\n",
    "y_train_t = (y_train * 2 + 1) / np.asarray(image_size) - 1\n",
    "y_train_res = np.concatenate((y_train_t, res_train), axis=1)\n",
    "y_val_t = (y_val * 2 + 1) / np.asarray(image_size) - 1\n",
    "y_val_res = np.concatenate((y_val_t, res_val), axis=1)\n",
    "y_test_t = (y_test * 2 + 1) / np.asarray(image_size) - 1\n",
    "y_test_res = np.concatenate((y_test_t, res_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0004ce24-2d83-409f-9e84-b1a21bd2a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, X, Y, transform=None, target_transform=None):\n",
    "        self.img = X\n",
    "        self.img_labels = Y\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.img[idx]\n",
    "        label = self.img_labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29546ff7-49f3-4869-831a-1b18dae141ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomImageDataset(x_train, y_train_res)\n",
    "val_dataset = CustomImageDataset(x_val, y_val_res)\n",
    "test_dataset = CustomImageDataset(x_test, y_test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6500778-c3dc-43dd-93b8-822d859a4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13600de5-9732-43ed-bbe0-3ea8d2e97fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([2, 1, 176, 88, 48])\n",
      "Labels batch shape: torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da2472d4-b736-41c6-86d9-386cc44213f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare Model\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "727350f2-a217-429b-a05b-c961a13f48fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv3d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.Conv3d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.Conv3d(16, 16, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf086d4e-f30d-428a-8444-2416e185f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 5\n",
    "\n",
    "class FCN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, kernel_size=kernel_size, padding=\"same\"),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(32, 64, kernel_size=kernel_size, padding=\"same\"),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(64, 128, kernel_size=kernel_size, padding=\"same\"),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(128, 64, kernel_size=kernel_size, padding=\"same\"),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(64, 128, kernel_size=kernel_size, padding=\"same\"),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(128, 64, kernel_size=kernel_size, padding=\"same\"),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "418a8624-255d-4ca6-aee0-fa0d765405b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dsntnn\n",
    "\n",
    "class CoordRegressionNetwork(nn.Module):\n",
    "    def __init__(self, n_locations):\n",
    "        super().__init__()\n",
    "        self.fcn = FCN1()\n",
    "        self.hm_conv = nn.Conv3d(64, n_locations, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # 1. Run the images through our FCN\n",
    "        fcn_out = self.fcn(images)\n",
    "        # 2. Use a 1x1 conv to get one unnormalized heatmap per location\n",
    "        unnormalized_heatmaps = self.hm_conv(fcn_out)\n",
    "        # 3. Normalize the heatmaps\n",
    "        heatmaps = dsntnn.flat_softmax(unnormalized_heatmaps)\n",
    "        # 4. Calculate the coordinates\n",
    "        coords = dsntnn.dsnt(heatmaps)\n",
    "\n",
    "        return coords, heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66cce3c0-df3f-4a8a-bbe4-53f57e059fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss Function\n",
    "def mse_with_res(y_true, y_pred, res):\n",
    "    \"\"\"\n",
    "    :param y_true: [batch_size, num_landmarks, dimension(column, row, slice)]\n",
    "    :param y_pred: [batch_size, num_landmarks, dimension(column, row, slice)]\n",
    "    :param res: Pixel distance in mm, [batch_size, 1, dimension(column, row, slice)]\n",
    "    :return: mean square error along batch_size (mm^2)\n",
    "    \"\"\"\n",
    "    err_diff = y_true - y_pred\n",
    "    # repeat res to make a convenient calculation follow\n",
    "    num_landmarks = err_diff.shape[1]\n",
    "    rep_res = torch.repeat_interleave(res, num_landmarks, axis=1)\n",
    "    # change pixel distance to mm (kind of normalization I think)\n",
    "    losses = err_diff\n",
    "    disses = err_diff * rep_res\n",
    "    square_losses = torch.pow(losses, 2)\n",
    "    square_disses = torch.pow(disses, 2 )\n",
    "    #loss = torch.mean(torch.sum(square_losses, (1, 2)))\n",
    "    loss = torch.sum(square_losses, (1, 2))\n",
    "    diss = torch.sum(square_disses, (1, 2))\n",
    "    return loss, diss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e72c8fc-f682-416d-ba94-ee701e8ef093",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoordRegressionNetwork(n_locations=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c898a00b-ab95-4ca5-ae25-1bb80049e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_var = train_features.cuda()\n",
    "coords, heatmaps = model(train_features_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9aac085-062c-4743-8eb2-9a56b49ae56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_labels[:, 0:2, :]\n",
    "res = train_labels[:, 2:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "196f39a6-5edd-490d-8be8-c8939a1cde21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2019, 0.2413], device='cuda:0', dtype=torch.float64,\n",
       "        grad_fn=<SumBackward1>),\n",
       " tensor([214.4305, 720.7372], device='cuda:0', dtype=torch.float64,\n",
       "        grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_var = y.cuda()\n",
    "res_var = res.cuda()\n",
    "mse_with_res(y_var, coords, res_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9084b8a-a600-47c9-a417-74377f341c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_pred, heatmaps, y_true_res):\n",
    "    y_true = y_true_res[:, 0:2, :]\n",
    "    res = y_true_res[:, 2:3, :]\n",
    "    \n",
    "    # Per-location euclidean losses\n",
    "    # euc_losses = dsntnn.euclidean_losses(coords, pts_tensor_var)\n",
    "    euc_losses, dist = mse_with_res(y_true, y_pred, res)\n",
    "    # Per-location regularization losses\n",
    "    reg_losses = dsntnn.js_reg_losses(heatmaps, y_true, sigma_t=1.0)\n",
    "    # Combine losses into an overall loss\n",
    "    loss = dsntnn.average_loss(euc_losses)\n",
    "    euc_mean = dsntnn.average_loss(dist)\n",
    "    reg_mean = dsntnn.average_loss(reg_losses)\n",
    "    return loss, euc_mean, reg_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd2a3379-c1bf-4f65-9d1f-04cb8d86bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "from torch import optim\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred, heat = model(X)\n",
    "        loss, dis, reg = loss_fn(pred, heat, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}] ({dis:>7f} / {reg:>5f})\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, dis_loss, reg_loss = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred, heat = model(X)\n",
    "            loss, dis, reg = loss_fn(pred, heat, y)\n",
    "            test_loss += loss.item()\n",
    "            dis_loss += dis.item()\n",
    "            reg_loss += reg.item()\n",
    "            #test_loss += loss_fn(pred, heat, y).item()\n",
    "    test_loss /= num_batches\n",
    "    dis_loss /= num_batches\n",
    "    reg_loss /= num_batches\n",
    "    print(f\"Test Error: \\nAvg loss: {test_loss:>8f}  Avg dis: {dis_loss:>8f}, Avg reg: {reg_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cea8bc9-9d8a-4fb7-aaa0-299343d7ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.419695  [    2/ 1400] (554.837249 / 0.691505)\n",
      "loss: 0.107535  [  202/ 1400] (233.992331 / 0.692585)\n",
      "loss: 0.111058  [  402/ 1400] (236.746304 / 0.690573)\n",
      "loss: 0.122123  [  602/ 1400] (196.004105 / 0.691762)\n",
      "loss: 0.173774  [  802/ 1400] (474.868352 / 0.692940)\n",
      "loss: 0.070097  [ 1002/ 1400] (195.062738 / 0.691855)\n",
      "loss: 0.095382  [ 1202/ 1400] (123.142464 / 0.691613)\n",
      "Test Error: \n",
      "Avg loss: 0.059504  Avg dis: 123.348871, Avg reg: 0.692208\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.165445  [    2/ 1400] (205.509148 / 0.692737)\n",
      "loss: 0.020747  [  202/ 1400] (96.841645 / 0.692279)\n",
      "loss: 0.028887  [  402/ 1400] (133.419003 / 0.693106)\n",
      "loss: 0.047193  [  602/ 1400] (79.883535 / 0.692400)\n",
      "loss: 0.078152  [  802/ 1400] (144.736482 / 0.692971)\n",
      "loss: 0.030027  [ 1002/ 1400] (52.583513 / 0.693089)\n",
      "loss: 0.104921  [ 1202/ 1400] (197.477901 / 0.693089)\n",
      "Test Error: \n",
      "Avg loss: 0.048914  Avg dis: 115.783780, Avg reg: 0.692330\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.036518  [    2/ 1400] (111.279005 / 0.691338)\n",
      "loss: 0.045202  [  202/ 1400] (144.668218 / 0.692690)\n",
      "loss: 0.030603  [  402/ 1400] (49.494410 / 0.692221)\n",
      "loss: 0.020655  [  602/ 1400] (81.453325 / 0.692873)\n",
      "loss: 0.018287  [  802/ 1400] (32.733096 / 0.692666)\n",
      "loss: 0.015256  [ 1002/ 1400] (72.226813 / 0.693147)\n",
      "loss: 0.012912  [ 1202/ 1400] (60.952066 / 0.693141)\n",
      "Test Error: \n",
      "Avg loss: 0.068590  Avg dis: 151.739670, Avg reg: 0.692906\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.048845  [    2/ 1400] (185.163872 / 0.693147)\n",
      "loss: 0.008525  [  202/ 1400] (15.722786 / 0.693090)\n",
      "loss: 0.049159  [  402/ 1400] (176.859605 / 0.693146)\n",
      "loss: 0.053732  [  602/ 1400] (194.173515 / 0.689613)\n",
      "loss: 0.073152  [  802/ 1400] (114.410355 / 0.693147)\n",
      "loss: 0.118574  [ 1002/ 1400] (199.301454 / 0.693085)\n",
      "loss: 0.058811  [ 1202/ 1400] (151.395418 / 0.693072)\n",
      "Test Error: \n",
      "Avg loss: 0.051130  Avg dis: 110.196425, Avg reg: 0.692678\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.017884  [    2/ 1400] (52.586966 / 0.693091)\n",
      "loss: 0.020121  [  202/ 1400] (40.705646 / 0.693030)\n",
      "loss: 0.003101  [  402/ 1400] (14.854060 / 0.693094)\n",
      "loss: 0.069031  [  602/ 1400] (138.539049 / 0.693106)\n",
      "loss: 0.019737  [  802/ 1400] (29.873096 / 0.693103)\n",
      "loss: 0.009594  [ 1002/ 1400] (23.121938 / 0.693094)\n",
      "loss: 0.020128  [ 1202/ 1400] (24.846503 / 0.693128)\n",
      "Test Error: \n",
      "Avg loss: 0.046022  Avg dis: 134.537592, Avg reg: 0.692836\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.012851  [    2/ 1400] (32.156780 / 0.689881)\n",
      "loss: 0.013087  [  202/ 1400] (70.955592 / 0.693124)\n",
      "loss: 0.014934  [  402/ 1400] (76.343260 / 0.693142)\n",
      "loss: 0.034684  [  602/ 1400] (134.362761 / 0.693147)\n",
      "loss: 0.013168  [  802/ 1400] (36.453446 / 0.693147)\n",
      "loss: 0.006596  [ 1002/ 1400] (31.614305 / 0.693128)\n",
      "loss: 0.029456  [ 1202/ 1400] (128.887237 / 0.693110)\n",
      "Test Error: \n",
      "Avg loss: 0.048935  Avg dis: 138.446800, Avg reg: 0.692798\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.009990  [    2/ 1400] (19.022668 / 0.690759)\n",
      "loss: 0.019155  [  202/ 1400] (39.503001 / 0.693113)\n",
      "loss: 0.016265  [  402/ 1400] (102.016685 / 0.692678)\n",
      "loss: 0.038012  [  602/ 1400] (229.430556 / 0.692433)\n",
      "loss: 0.015101  [  802/ 1400] (79.862133 / 0.693144)\n",
      "loss: 0.012716  [ 1002/ 1400] (67.127766 / 0.693142)\n",
      "loss: 0.009641  [ 1202/ 1400] (16.870702 / 0.693137)\n",
      "Test Error: \n",
      "Avg loss: 0.047834  Avg dis: 116.970668, Avg reg: 0.693045\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.013214  [    2/ 1400] (37.983507 / 0.693146)\n",
      "loss: 0.004426  [  202/ 1400] (16.153746 / 0.693123)\n",
      "loss: 0.004435  [  402/ 1400] (12.782319 / 0.692970)\n",
      "loss: 0.005663  [  602/ 1400] (13.798234 / 0.693049)\n",
      "loss: 0.035288  [  802/ 1400] (127.940290 / 0.691335)\n",
      "loss: 0.031063  [ 1002/ 1400] (98.290000 / 0.693137)\n",
      "loss: 0.004175  [ 1202/ 1400] (14.304533 / 0.690275)\n",
      "Test Error: \n",
      "Avg loss: 0.046853  Avg dis: 120.178679, Avg reg: 0.692836\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.009682  [    2/ 1400] (22.984236 / 0.693127)\n",
      "loss: 0.008792  [  202/ 1400] (15.721023 / 0.693138)\n",
      "loss: 0.003836  [  402/ 1400] (7.723496 / 0.693146)\n",
      "loss: 0.025198  [  602/ 1400] (179.784475 / 0.693147)\n",
      "loss: 0.025642  [  802/ 1400] (45.823734 / 0.693134)\n",
      "loss: 0.024545  [ 1002/ 1400] (94.618308 / 0.693115)\n",
      "loss: 0.005102  [ 1202/ 1400] (15.407073 / 0.693075)\n",
      "Test Error: \n",
      "Avg loss: 0.050475  Avg dis: 131.379627, Avg reg: 0.692919\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.027760  [    2/ 1400] (93.073644 / 0.693130)\n",
      "loss: 0.020436  [  202/ 1400] (60.996289 / 0.693147)\n",
      "loss: 0.007777  [  402/ 1400] (31.148546 / 0.693147)\n",
      "loss: 0.023996  [  602/ 1400] (99.963306 / 0.693118)\n",
      "loss: 0.005296  [  802/ 1400] (10.019017 / 0.690975)\n",
      "loss: 0.026470  [ 1002/ 1400] (141.610262 / 0.693147)\n",
      "loss: 0.009010  [ 1202/ 1400] (37.914517 / 0.691350)\n",
      "Test Error: \n",
      "Avg loss: 0.041382  Avg dis: 111.314902, Avg reg: 0.693030\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = CoordRegressionNetwork(n_locations=2).to(device)\n",
    "\n",
    "opt_rms = optim.RMSprop(model.parameters(), lr=0.0001)\n",
    "opt_ada = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, opt_ada)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd046ad-ba1a-4c27-9116-b3fd08c796bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
