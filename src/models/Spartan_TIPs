sinteractive -p gpu-a100 --gres=gpu:1 -A ***
# sinteractive -p gpu-a100 --gres=gpu:1 --mem=30G --time=(01:30:00?)


-p: partition
--gres: generic resources
-A: account group, e.g.punim0006
--time 00:05:00


module purge
module load CUDA/11.7.0
module load GCC/11.3.0  OpenMPI/4.1.4
module load TensorFlow/2.11.0-CUDA-11.7.0


module load CUDA/11.7.0 SciPy-bundle/2022.05 Pillow/9.1.1

# It seems this version was not compiled to use GPU
module load tensorflow/2.7.1

***Slurm***
# Slurm Environmental Variables
https://hpcc.umd.edu/hpcc/help/slurmenv.html

scontrol write batch_script <job_id>

***Python***

import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))


# To show which device dit the operations
tf.debugging.set_log_device_placement(True)

# Data Parallelism basics?
## 1. divide batch: batch-size=10, 2 GPUs, each batch of size 10 will be divided among the 2 GPUs, with each receiving 5 input examples in each step.

*** Conda 
# activate Env
. /Users/achs/opt/anaconda3/bin/activate && conda activate /Users/achs/opt/anaconda3/envs/test;

# use pip to install in conda
https://stackoverflow.com/questions/41060382/using-pip-to-install-packages-to-anaconda-environment

# spartan write batch script
scontrol write batch_script work_id

# spartan login node
module load Anaconda3/2024.02-1
eval "$(conda shell.bash hook)"
conda activate first_env
pip install jupyterlab notebook

# spartan virtual desktop
## job id can be found in the session output.log
sacct -j [job-id] --format alloctres%50,workdir%150

# unimelb spartan jupyter
https://dashboard.hpc.unimelb.edu.au/webenvironments/openondemand/jupyter/
## Commands to run before Jupyter
module load CUDA/11.7.0;conda activate first_env
## Partition
gpu-a100

# jupyter notebook (kernel)
jupyter kernelspec list
# ipython
https://ipython.readthedocs.io/en/latest/install/kernel_install.html#kernels-for-different-environments


*** Python crop and combine
import TrainingSupport
import numpy as np

base_dir = "/data/gpfs/projects/punim1836/Data/cropped/based_on_truth/x100100y100100z8080"

x_dir = base_dir + "/volumes/"

y_dir = base_dir + "/points/"

length_dir = base_dir + "/length/"

TrainingSupport.load_dataset_crop_dir(x_dir,y_dir,length_dir)

import importlib

importlib.reload(support_modules)

*** Python tmp


